---
title: "hw3_p2_churn"
author: "kchaudhari"
date: "2024-02-02"
output:
 html_document: 
   smart: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This problem is based on one of [Kaggle's Playground Series of competitions](https://www.kaggle.com/docs/competitions). The Playground Series is a nice way to practice building predictive models by "providing interesting and approachable datasets for our community to practice their machine learning skills". 

You do **NOT** need to download any data from Kaggle. I've created a smaller dataset with some other modifications for use in our HW problem. The datafile, `churn.csv`, is available in the `data` subfolder.

This particular [playground dataset involves data about bank customers](https://www.kaggle.com/competitions/playground-series-s4e1) with the target variable being a binary indicator of whether or not the customer left the bank (`Exited`), or "churned". The playground dataset was constructed using another [Kaggle dataset on bank customer churn prediction](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction). Follow the preceeding link for information about the variables in this dataset. 

This assignment will focus on building simple classification models for
predicting bank customer churn. You'll be doing your work right in this R Markdown document. Feel free to save it first with a modified filename that includes your name. For example, mine would be **hw3_p2_churn_isken.Rmd**.

You'll likely need a bunch of libraries. I've included a few here but you should add any others that you need. If you don't need some of these, feel free to delete such lines.

```{r}
library(dplyr)   # Group by analysis and other SQLish things.
library(ggplot2) # Plotting, of course
library(corrplot) # Correlation plots
library(caret)   # Many aspects of predictive modeling
library(skimr)  # An automated EDA tool 
```
**MAJOR (10%) HACKER EXTRA** Version control

Create a new R Project for this assignment. Put the project under version control with git. Create a private GitHub repository for this project. Use git and GitHub as you go to do commits periodically and push them to your remote repository. After you have completed the assignment and pushed your last commit to your GitHub repo, add me as a Collaborator (my GitHub username is misken) so that I can see your repo.

I cover use of git and GitHub with R Studio in this module on our course web page:

* [http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html](http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html)

This Hacker Extra is worth 10% of the total number of points in the assignment.

## Step 1: Read in data

Read the `churn.csv` file from the `data` subfolder into a dataframe named `churn`.

```{r read_churn}

churn <- read.csv("data/churn.csv")
```

Use `str`, `summary`, and `skim` to get a sense of the data. 
The binary target variable is `Exited` where 1 indicates that the customer left the bank (they "churned"). You'll notice that some of the fields are numeric and some are character data. You might also notice that there are fewer variables in our churn dataset than in the original Kaggle versions.
```{r first_look}
str(churn)
summary(churn)
skim(churn)
```

## Step 2: Factor conversions

Some of the variables clearly should be factors. Change all of the variables to factors that you think should be. Include an explanation of why each of these variables should be converted to factors.

```{r conversions}
# List of column names to convert to factors
columns_to_convert <- c("Geography", "Gender", "HasCrCard", "IsActiveMember", "Exited")

# Convert selected columns to factors using lapply
churn[columns_to_convert] <- lapply(churn[columns_to_convert], factor)

```

## Step 3 - Partition into training and test sets

We will use the [caret](https://topepo.github.io/caret/) package to do the partitioning of our data into training and test dataframes. Just run this chunk to create training and test datasets. This way we'll all be working with the same datasets. Notice that the test set is 20% of the full dataset.

```{r partition}
# Simple partition into train (80%) and test (20%) set 
set.seed(687)
trainIndex <- createDataPartition(churn$Exited, p = .8, 
                                  list = FALSE, 
                                  times = 1)

churn_train <- churn[as.vector(trainIndex), ]  
churn_test <- churn[-as.vector(trainIndex), ]

```

Find the number of customers and the percentage of customers for the two `Exited` levels. You'll
see that there are about 20% of the bank customers exited.

```{r target_prop_check_train}
exit_counts <- table(churn$Exited)
exit_counts
exit_percentages <- prop.table(exit_counts) * 100
exit_percentages
```


## Step 4: EDA

Do some EDA to try to uncover some relationships that may end up being
useful in building a predictive model for `Exited`. You learned
things in HW2 which should be useful here. You should **ONLY** use `churn_train` for your EDA. You should explore all of the variables.
```{r summary statistics}
# Summary statistics for numeric variables
summary(churn_train[, c("CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "EstimatedSalary")])

# Summary statistics for categorical variables
summary(churn_train[, c("Geography", "Gender", "HasCrCard", "IsActiveMember", "Exited")])

#  Histograms for numeric variables
hist(churn_train$Age, main = "Distribution of Age")

#  Bar plots for categorical variables
ggplot(churn_train, aes(x = Geography)) + geom_bar() + labs(title = "Distribution of Geography")

# Boxplot of Age by Exited
ggplot(churn_train, aes(x = Exited, y = Age)) + geom_boxplot() + labs(title = "Age Distribution by Exited")

# Bar plot of Exited by Gender
ggplot(churn_train, aes(x = Exited, fill = Gender)) + geom_bar(position = "fill") + labs(title = "Exited Distribution by Gender")

# Correlation analysis
correlation_matrix <- cor(churn_train[, c("CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "EstimatedSalary")])
corrplot::corrplot(correlation_matrix, method = "color")

```

## Step 5 - Building and evaluation of predictive classification models

Now that you know a little more about the data, it's time to start building a
few classification models for `Exited`. We will start out using overall prediction accuracy
as our metric but we might want to consider other metrics.

**QUESTION** Why might overall prediction accuracy not be the most appropriate metric to consider? What other metrics might be important and why?

> If one class dominates the dataset, it will naturally result in high accuracy even if all predictions are done as majority class. Like non exited who are 80%. Even if all are non exited still the accuracy will be 80% but we will miss the crucial 20% whom we are targetting for analysis. 
Other metris which are important are:   
Accuracy:It focuses on the accuracy of positive predictions and is useful when the cost of false positives is high.   
Sensitivity: It measures the model's ability to correctly identify positive instances  
Specificity: The proportion of true negative predictions out of all actual negative instances in the data. 

### Fit a null model

A very simple model would be to simply predict that `Exited` is equal to 0. On
the training data we saw that we'd be ~80% accurate.

Let's create this null model and run a confusion matrix on its "predictions" for both the training
and the test data.

```{r tree_null}
# Create a vector of 0's
model_train_null <- rep(0, nrow(churn_train))
model_test_null <- rep(0, nrow(churn_test))

cm_train_null <- caret::confusionMatrix(as.factor(model_train_null), 
                                        churn_train$Exited, 
                                        positive = "1")
cm_train_null

cm_test_null <- caret::confusionMatrix(as.factor(model_test_null), 
                                       churn_test$Exited, 
                                       positive = "1")
cm_test_null
```

**QUESTION** A few questions:

* Are you surprised that the performance of the null model is almost identical on test and train? Why or why not?
* Explain the sensitivity and specificity values.

> No, it's not surprising that the performance of the null model is almost identical on both the test and train datasets. The null model simply predicts the majority class (0 in this case, indicating customers who did not exit the bank) for all instances, resulting in a constant prediction regardless of the dataset. Since the null model does not learn from the data or make any distinctions between train and test sets, its performance remains consistent across both datasets.  
Sensitivity - Sensitivity measures the ability of the model to correctly identify positive cases out of all actual positive cases. Since the null model predicts all cases as negative (0), it correctly identifies none of the actual positive cases. Therefore, the sensitivity value is 0.  
Specificity - Specificity measures the ability of the model to correctly identify negative cases out of all actual negative cases. Since the null model predicts all cases as negative (0), it correctly identifies all actual negative cases. Therefore, the specificity value is 1.

So, as we begin fitting more complicated models, remember that we need to
outperform the null model to make it worth it to use more complicated models.

Now I'm going to ask you to fit three models:

* a logistic regression model
* a simple decision tree
* a random forest

We covered all three of these modeling techniques in the class notes.

For each model type, you should:

* fit the model on the training data,
* assess the model's performance on the training data using the `confusionMatrix` function,
* use the model to make predictions on the test data,
* assess the model's performance on the test data using the `confusionMatrix` function,
* discuss the results

In your discussion of the results you should talk about things like:

* how accurate is the model in predicting on the test data
* is there evidence of overfitting?
* how does the model do in terms of other metrics like sensitivity and specificity
* other things you deem important.

### Fit logistic regression models

You'll start by creating a logistic regression model to predict `Exited`. Since there
are not that many variables, let's use all of them. Here's a code skeleton to help you get started:

**Hint**: There's an easy way to specify your model formula to include all of the predictor variables
without typing out all the variable names. 

```{r lr1_train}
# Fit model to training data
model_lr1 <- glm(Exited ~ ., 
                 data=churn_train, 
                 family=binomial(link="logit"))

## Convert fitted model values to fitted classes. Use 0.5 as the
#  threshold for classifying a case as a 1.

class_train_lr1 <- as.factor((model_lr1$fit > 0.5)*1)

cm_train_lr1 <- confusionMatrix(as.factor(class_train_lr1), 
                                churn_train$Exited, 
                                positive = "1")

cm_train_lr1
```

Now, let's predict on test data.

```{r lr1_test}

pred_lr1 <- predict(model_lr1, newdata = churn_test, type = "response")

class_test_lr1 <- as.factor((pred_lr1 > 0.5)*1)
                          
cm_test_lr1 <- confusionMatrix(class_test_lr1, churn_test$Exited, positive = "1")
cm_test_lr1

```

**QUESTION** How did accuracy, sensitivity and specificity change when predicting on test data instead of the training data?

> All three decreased slighlty from training to test data.This may be because the model is being evaluated on unseen data, and it may not generalize as well as it did on the training data.
Accuracy: 0.8363 to 0.832  
Sensitivity: 0.38375 to 0.37581  
Specificity: 0.95613 to 0.95267  

Now change the threshold from 0.5 to 0.4 and create a new model using this new threshold. How does the sensitivity and specificity change as compared to our first logistic regression model? Explain why this happens?

```{r increase_sensitivity}
model_lr2 <- glm(Exited ~ ., 
                 data=churn_train, 
                 family=binomial(link="logit"))

## Convert fitted model values to fitted classes. Use 0.4 as the
#  threshold for classifying a case as a 1.

class_train_lr2 <- as.factor((model_lr2$fit > 0.4)*1)

cm_train_lr2 <- confusionMatrix(as.factor(class_train_lr2), 
                                churn_train$Exited, 
                                positive = "1")
cm_train_lr2

#predict test data
pred_lr2 <- predict(model_lr2, newdata = churn_test, type = "response")

class_test_lr2 <- as.factor((pred_lr2 > 0.4)*1)
                          
cm_test_lr2 <- confusionMatrix(class_test_lr2, churn_test$Exited, positive = "1")
cm_test_lr2

```

> We see the senstivity go up and specificity go down from the previous model of 0.5. 
Basically when we change the threshold from 0.5 to 0.4, we are lowering the threshold, which may cause  to classify more cases as positive (i.e. predicting that customers will churn). This increases our sensitivity, which means we're better at detecting the customers who are actually going to churn.   However, this also tends to decrease specificity because we're now more likely to classify some non-churners as churners. 


### Fit simple decision tree model

Now create a simple decision tree model to predict `Exited`. Again,
use all the variables.

```{r tree1_train}

# Load the required library for decision trees
library(rpart)

# Fit decision tree model to training data
model_tree1 <- rpart(Exited ~ ., data = churn_train)

# Predict classes for the training data
class_train_tree1 <- predict(model_tree1, type = "class")

# Convert predicted classes to factors
class_train_tree1 <- as.factor(class_train_tree1)

# Compute confusion matrix for the training data
cm_train_tree1 <- confusionMatrix(class_train_tree1, churn_train$Exited, positive = "1")
cm_train_tree1


```

Create a plot of your decision tree.

```{r decision_tree_plot}
# Load the required library for plotting decision trees
library(rpart.plot)

# Plot the decision tree
rpart.plot(model_tree1, yesno = 2)

```

Explain the bottom left node of your tree. What conditions have to be true for a case to end up being classified by that node? What do those three numbers in the node mean? What does the color of the node mean?

> Bottom left node (no of products>3) conditions for customer to be classified true: age < 43 and no. of products used >3  and exited:Yes
Numbers meaning: First number shows the result 0 or 1 which means either the customer exit/did not exit.   
Second number shows the exit probability.  
Third number indicates proportion of the population that resides in this node.   
Color are given based on the outcomes(i.e. exited here) Blue one are those who exited while green ones are those who stay

Now, let's predict on test data.

```{r tree1_test}

# Predict classes for the test data
pred_tree1 <- predict(model_tree1, newdata = churn_test, type = "class")
pred_tree1 <- as.factor(pred_tree1)

# Confusion matrix for the test data
cm_test_tree1 <- confusionMatrix(pred_tree1, churn_test$Exited, positive = "1")
cm_test_tree1


```

**QUESTION** How does the performance of the decision tree compare to your
logistic regression model? 

> The perfomance measure look similar for both the models. Accuracy, sensitivity decreased slightly and specificity reamined the same

## Fit random forest model

Finally, fit a random forest model.

```{r rf1_train}
# Load the required library for random forests
library(randomForest)

# Fit random forest model to training data
model_rf1 <- randomForest(Exited ~ ., data = churn_train)

class_train_rf1 <- predict(model_rf1, type = "class")
class_train_rf1 <- as.factor(class_train_rf1)

cm_train_rf1 <- confusionMatrix(class_train_rf1, churn_train$Exited, positive = "1")
cm_train_rf1

```

Now, let's predict on test data.

```{r rf1_test}
# Fit random forest model to test data
model_rf1 <- randomForest(Exited ~ ., data = churn_test)

class_test_rf1 <- predict(model_rf1, type = "class")
class_test_rf1 <- as.factor(class_test_rf1)

cm_test_rf1 <- confusionMatrix(class_test_rf1, churn_test$Exited, positive = "1")
cm_test_rf1


```


**QUESTION** Summarize the performance of all three of your models (logistic, tree, random forest)? Is their evidence of overfitting in any of these model and what is your evidence for your answer? Add code chunks as needed.

> Out of All there models highest test accuracy is for the tree model 85.26% but less sensitivity 0.4366. However the best sensitivity is for the random forest model 0.4982 with reasonable accuracy 85.08%. Logistics model (0.4) also performs good in sensitivity 0.4982 but with reduced accuracy 83.09%.   
There is a chance of overfitting in random forest model as the accuracy has dropped from train to test results. 


**QUESTION** If you had to pick one to use in an actual financial environment, which model would you use and why? As a manager in charge of retention, what model performance metrics are you most interested in? What are the basic tradeoffs you see in terms of the initiatives you might undertake in response to such a model? For example, if you were really interested in reducing the number of customers exiting, maybe there are some things you might do to incent high risk (of exiting) customers to stay. Discuss.

> : I would choose the random forest model which shows highest accuracy and sensitivity. This indicates that it is better at correctly identifying both customers who are likely to churn and those who are not.  
As a manager in charge of retention, the model performance metrics that I would be most interested in are sensitivity This metric measures the proportion of actual churn cases that are correctly identified by the model. As a retention manager, my primary goal would be to accurately identify customers who are at risk of churning so that appropriate retention strategies can be implemented.
We need to find the at risk customers first to find how to incentivise them. For example if age is a factor what should we do differently. e.g if age>43 are exiting then we need to study these groups and find ways to retain them. Similarly geography can be a factor. 

**HACKER EXTRA**

Create a variable importance plot for your random forest to try to get a sense of which variables are most important in predicting customers likely to churn. Build another random forest using only the top 5 or so variables suggested by the importance plot. How does the performance of this reduced model compare to the original model?

```{r importance}
# Obtain variable importance from the random forest model
importance <- randomForest::importance(model_rf1)

# Create a variable importance plot
varImpPlot(model_rf1)

# Select the top variables based on their importance scores
top_vars <- rownames(importance)[order(importance[, "MeanDecreaseGini"], decreasing = TRUE)][1:5]

# Build a new random forest model using only the selected variables
model_rf_reduced <- randomForest(Exited ~ ., data = churn_train[, c("Exited", top_vars)])

# Predict on test data using the reduced model
class_test_rf_reduced <- predict(model_rf_reduced, newdata = churn_test[, top_vars], type = "class")

# Compute confusion matrix for the reduced model
cm_test_rf_reduced <- confusionMatrix(class_test_rf_reduced, churn_test$Exited, positive = "1")
cm_test_rf_reduced
```

> The accuracy of this model is 84.38 while the sensitivity is reduced to 0.46. The accuracy and senstivity has reduced when compared to the top performing model for each